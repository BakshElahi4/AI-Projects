{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCPhfmun3nfZ",
        "outputId": "2d47acd7-9b38-4953-edb5-5f7db131a56b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "print(os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch as t\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "VJVnSSz0i1tG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch as t  # optional, used later in ML if needed\n",
        "\n",
        "# Step 1: Define the zip file path\n",
        "zip_path = '/content/Ohio Data.zip'\n",
        "\n",
        "# Step 2: Define the extraction target path\n",
        "extract_to = '/content'\n",
        "\n",
        "# Step 3: Extract if not already done\n",
        "extracted_main_folder = os.path.join(extract_to, 'Ohio Data')  # because zip contains \"Ohio Data/\" folder\n",
        "if not os.path.exists(extracted_main_folder):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)  # extracts to /content/Ohio Data\n",
        "\n",
        "# Step 4: Define paths to 2018 and 2020 data folders\n",
        "folder_2018 = os.path.join(extracted_main_folder, 'Ohio2018_processed')\n",
        "folder_2020 = os.path.join(extracted_main_folder, 'Ohio2020_processed')\n",
        "\n",
        "# Step 5: Print check info\n",
        "print(\"âœ… Folders Exist?\")\n",
        "print(\"2018 Folder:\", folder_2018, \"| Exists:\", os.path.exists(folder_2018))\n",
        "print(\"2020 Folder:\", folder_2020, \"| Exists:\", os.path.exists(folder_2020))\n",
        "\n",
        "# Step 6: List contents of main folders\n",
        "print(\"\\nðŸ“‚ Contents of Ohio2018_processed:\", os.listdir(folder_2018))\n",
        "print(\"ðŸ“‚ Contents of Ohio2020_processed:\", os.listdir(folder_2020))\n",
        "\n",
        "# Step 7: Define train/test paths\n",
        "train_2018_path = os.path.join(folder_2018, 'train')\n",
        "test_2018_path = os.path.join(folder_2018, 'test')\n",
        "train_2020_path = os.path.join(folder_2020, 'train')\n",
        "test_2020_path = os.path.join(folder_2020, 'test')\n",
        "\n",
        "# Step 8: List CSV files in each\n",
        "train_files_2018 = os.listdir(train_2018_path)\n",
        "test_files_2018 = os.listdir(test_2018_path)\n",
        "train_files_2020 = os.listdir(train_2020_path)\n",
        "test_files_2020 = os.listdir(test_2020_path)\n",
        "\n",
        "print(\"\\nðŸ“„ Train 2018 files:\", train_files_2018)\n",
        "print(\"ðŸ“„ Test 2018 files:\", test_files_2018)\n",
        "print(\"ðŸ“„ Train 2020 files:\", train_files_2020)\n",
        "print(\"ðŸ“„ Test 2020 files:\", test_files_2020)\n",
        "\n",
        "# Step 9: Load one training CSV from 2018\n",
        "sample_file_path = os.path.join(train_2018_path, train_files_2018[0])\n",
        "sample_data = pd.read_csv(sample_file_path)\n",
        "\n",
        "# Step 10: Show the last 100 rows\n",
        "print(\"\\nðŸ“Š Sample CSV preview (last 100 rows):\")\n",
        "sample_data.tail(100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "id": "8KiH97UK4d8R",
        "outputId": "9de78e0f-ed7c-4e43-c40c-7fbc6f2b3958"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Folders Exist?\n",
            "2018 Folder: /content/Ohio Data/Ohio2018_processed | Exists: True\n",
            "2020 Folder: /content/Ohio Data/Ohio2020_processed | Exists: True\n",
            "\n",
            "ðŸ“‚ Contents of Ohio2018_processed: ['train', 'test']\n",
            "ðŸ“‚ Contents of Ohio2020_processed: ['train', 'test']\n",
            "\n",
            "ðŸ“„ Train 2018 files: ['588-ws-training_processed.csv', '559-ws-training_processed.csv', '563-ws-training_processed.csv', '570-ws-training_processed.csv', '575-ws-training_processed.csv', '591-ws-training_processed.csv']\n",
            "ðŸ“„ Test 2018 files: ['588-ws-testing_processed.csv', '559-ws-testing_processed.csv', '570-ws-testing_processed.csv', '575-ws-testing_processed.csv', '563-ws-testing_processed.csv', '591-ws-testing_processed.csv']\n",
            "ðŸ“„ Train 2020 files: ['567-ws-training_processed.csv', '544-ws-training_processed.csv', '540-ws-training_processed.csv', '552-ws-training_processed.csv', '596-ws-training_processed.csv', '584-ws-training_processed.csv']\n",
            "ðŸ“„ Test 2020 files: ['552-ws-testing_processed.csv', '540-ws-testing_processed.csv', '596-ws-testing_processed.csv', '584-ws-testing_processed.csv', '567-ws-testing_processed.csv', '544-ws-testing_processed.csv']\n",
            "\n",
            "ðŸ“Š Sample CSV preview (last 100 rows):\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       5minute_intervals_timestamp  missing_cbg    cbg  finger  basal    hr  \\\n",
              "13005                 5.447421e+06          0.0  102.0     NaN   1.25  98.0   \n",
              "13006                 5.447422e+06          0.0  110.0     NaN   1.25  79.0   \n",
              "13007                 5.447423e+06          0.0  118.0     NaN   1.25  75.0   \n",
              "13008                 5.447424e+06          0.0  125.0     NaN   1.25  77.0   \n",
              "13009                 5.447425e+06          0.0  128.0     NaN   1.25  83.0   \n",
              "...                            ...          ...    ...     ...    ...   ...   \n",
              "13100                 5.447516e+06          0.0  150.0     NaN   1.25  63.0   \n",
              "13101                 5.447517e+06          0.0  144.0     NaN   1.25  63.0   \n",
              "13102                 5.447518e+06          0.0  140.0     NaN   1.25  63.0   \n",
              "13103                 5.447519e+06          0.0  137.0     NaN   1.25  60.0   \n",
              "13104                 5.447520e+06          0.0  132.0     NaN   1.25  59.0   \n",
              "\n",
              "            gsr  carbInput  bolus  \n",
              "13005  0.000059        NaN    NaN  \n",
              "13006  0.000057        NaN    NaN  \n",
              "13007  0.000058        NaN    NaN  \n",
              "13008  0.000058        NaN    NaN  \n",
              "13009  0.000057        NaN    NaN  \n",
              "...         ...        ...    ...  \n",
              "13100  5.370000        NaN    NaN  \n",
              "13101  6.562000        NaN    NaN  \n",
              "13102  9.904000        NaN    NaN  \n",
              "13103  8.846000        NaN    NaN  \n",
              "13104  7.752000        NaN    NaN  \n",
              "\n",
              "[100 rows x 9 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-275e5e64-03a1-4bb0-9b60-f294a3132e13\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>5minute_intervals_timestamp</th>\n",
              "      <th>missing_cbg</th>\n",
              "      <th>cbg</th>\n",
              "      <th>finger</th>\n",
              "      <th>basal</th>\n",
              "      <th>hr</th>\n",
              "      <th>gsr</th>\n",
              "      <th>carbInput</th>\n",
              "      <th>bolus</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>13005</th>\n",
              "      <td>5.447421e+06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>102.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.25</td>\n",
              "      <td>98.0</td>\n",
              "      <td>0.000059</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13006</th>\n",
              "      <td>5.447422e+06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>110.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.25</td>\n",
              "      <td>79.0</td>\n",
              "      <td>0.000057</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13007</th>\n",
              "      <td>5.447423e+06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>118.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.25</td>\n",
              "      <td>75.0</td>\n",
              "      <td>0.000058</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13008</th>\n",
              "      <td>5.447424e+06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>125.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.25</td>\n",
              "      <td>77.0</td>\n",
              "      <td>0.000058</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13009</th>\n",
              "      <td>5.447425e+06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>128.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.25</td>\n",
              "      <td>83.0</td>\n",
              "      <td>0.000057</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13100</th>\n",
              "      <td>5.447516e+06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.25</td>\n",
              "      <td>63.0</td>\n",
              "      <td>5.370000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13101</th>\n",
              "      <td>5.447517e+06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>144.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.25</td>\n",
              "      <td>63.0</td>\n",
              "      <td>6.562000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13102</th>\n",
              "      <td>5.447518e+06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>140.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.25</td>\n",
              "      <td>63.0</td>\n",
              "      <td>9.904000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13103</th>\n",
              "      <td>5.447519e+06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>137.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.25</td>\n",
              "      <td>60.0</td>\n",
              "      <td>8.846000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13104</th>\n",
              "      <td>5.447520e+06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>132.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.25</td>\n",
              "      <td>59.0</td>\n",
              "      <td>7.752000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows Ã— 9 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-275e5e64-03a1-4bb0-9b60-f294a3132e13')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-275e5e64-03a1-4bb0-9b60-f294a3132e13 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-275e5e64-03a1-4bb0-9b60-f294a3132e13');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-d0d5ab6e-bac7-4d29-ad5a-710a60b61f23\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d0d5ab6e-bac7-4d29-ad5a-710a60b61f23')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-d0d5ab6e-bac7-4d29-ad5a-710a60b61f23 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"sample_data\",\n  \"rows\": 100,\n  \"fields\": [\n    {\n      \"column\": \"5minute_intervals_timestamp\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 29.01459149439032,\n        \"min\": 5447420.989423077,\n        \"max\": 5447520.0,\n        \"num_unique_values\": 100,\n        \"samples\": [\n          5447503.998290598,\n          5447473.99508547,\n          5447490.99690171\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"missing_cbg\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cbg\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 33.9092907622675,\n        \"min\": 101.0,\n        \"max\": 234.0,\n        \"num_unique_values\": 69,\n        \"samples\": [\n          132.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"finger\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 57.45142876088171,\n        \"min\": 112.0,\n        \"max\": 239.0,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          123.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"basal\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.43591886002472163,\n        \"min\": 0.0,\n        \"max\": 1.25,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"hr\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 19.111577842103507,\n        \"min\": 59.0,\n        \"max\": 130.0,\n        \"num_unique_values\": 45,\n        \"samples\": [\n          63.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"gsr\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.7638634127893404,\n        \"min\": 5.6e-05,\n        \"max\": 9.904,\n        \"num_unique_values\": 28,\n        \"samples\": [\n          0.000256\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"carbInput\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 21.920310216782973,\n        \"min\": 14.0,\n        \"max\": 45.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          14.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"bolus\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 2.5,\n        \"max\": 2.5,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          2.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.interpolate import CubicSpline\n",
        "\n",
        "def get_preprocessor(data_df):\n",
        "    data_df1 = data_df.copy()\n",
        "\n",
        "    # Replace missing cbg values (where 'missing_cbg' == 1) using cubic spline interpolation\n",
        "    if 'cbg' in data_df1.columns and 'missing_cbg' in data_df1.columns:\n",
        "        valid_indices = data_df1[data_df1['missing_cbg'] == 0].index\n",
        "        valid_timestamps = data_df1.loc[valid_indices, '5minute_intervals_timestamp']\n",
        "        valid_cbg = data_df1.loc[valid_indices, 'cbg']\n",
        "\n",
        "        # Apply spline interpolation only if we have enough points\n",
        "        if len(valid_cbg) > 3:\n",
        "            spline = CubicSpline(valid_timestamps, valid_cbg)\n",
        "            missing_indices = data_df1[data_df1['missing_cbg'] == 1].index\n",
        "            missing_timestamps = data_df1.loc[missing_indices, '5minute_intervals_timestamp']\n",
        "            data_df1.loc[missing_indices, 'cbg'] = spline(missing_timestamps)\n",
        "\n",
        "    # Move 'cbg' to the end\n",
        "    cbg = data_df1.pop('cbg')\n",
        "    data_df1 = data_df1.assign(cbg=cbg)\n",
        "\n",
        "    # Drop time column\n",
        "    data_df1 = data_df1.drop(columns=['5minute_intervals_timestamp'])\n",
        "\n",
        "    # Fill NaNs with (min - 1% of abs(min))\n",
        "    column_mins = data_df1.min()\n",
        "    fill_values = column_mins - 0.01 * np.abs(column_mins)\n",
        "    data_df2 = data_df1.fillna(fill_values)\n",
        "\n",
        "    # Fit MinMaxScaler\n",
        "    scaler = MinMaxScaler()\n",
        "    scaler.fit(data_df2)\n",
        "\n",
        "    return scaler"
      ],
      "metadata": {
        "id": "wbBIIDKY4lQB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.interpolate import CubicSpline\n",
        "\n",
        "def interpolate_cbg(data_df):\n",
        "    data_df = data_df.copy()\n",
        "\n",
        "    # Perform cubic spline interpolation on 'cbg' where 'missing_cbg' == 1\n",
        "    missing_cbg_indices = data_df[data_df['missing_cbg'] == 1].index\n",
        "    valid_indices = data_df[data_df['missing_cbg'] == 0].index\n",
        "\n",
        "    if len(valid_indices) > 3:  # CubicSpline needs > 3 points\n",
        "        cs = CubicSpline(valid_indices, data_df.loc[valid_indices, 'cbg'])\n",
        "        data_df.loc[missing_cbg_indices, 'cbg'] = cs(missing_cbg_indices)\n",
        "\n",
        "    return data_df\n",
        "\n",
        "def move_cbg_to_end(df):\n",
        "    cbg = df.pop('cbg')\n",
        "    return df.assign(cbg=cbg)\n",
        "\n",
        "def get_scaler(data_df):\n",
        "    df = interpolate_cbg(data_df)\n",
        "\n",
        "    # Drop unnecessary columns if they exist\n",
        "    df = df.drop(columns=[col for col in ['5minute_intervals_timestamp', 'missing_cbg', 'index'] if col in df.columns])\n",
        "\n",
        "    df = move_cbg_to_end(df)\n",
        "\n",
        "    column_mins = df.min()\n",
        "    fill_values = column_mins - 0.01 * np.abs(column_mins)\n",
        "    df_filled = df.fillna(fill_values)\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    scaler.fit(df_filled)\n",
        "\n",
        "    return scaler, fill_values\n",
        "\n",
        "def preprocess(scaler, fill_values, data_df):\n",
        "    df = interpolate_cbg(data_df)\n",
        "\n",
        "    df = df.drop(columns=[col for col in ['5minute_intervals_timestamp', 'missing_cbg', 'index'] if col in df.columns])\n",
        "    df = move_cbg_to_end(df)\n",
        "\n",
        "    values = df.values\n",
        "    values = np.where(np.isnan(values), fill_values.values, values)\n",
        "\n",
        "    # Final check\n",
        "    if np.isnan(values).sum() > 0:\n",
        "        raise ValueError(\"NaNs still present after fill.\")\n",
        "\n",
        "    df_filled = pd.DataFrame(values, columns=df.columns)\n",
        "    df_scaled = pd.DataFrame(scaler.transform(df_filled), columns=df.columns)\n",
        "\n",
        "    return df_scaled\n"
      ],
      "metadata": {
        "id": "tRGRvVeQigzz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import torch as t\n",
        "\n",
        "class OhioT1DMDataset(Dataset):\n",
        "    def __init__(self, data_dirs, seq_length):\n",
        "        self.seq_length = seq_length\n",
        "        dataframes = []\n",
        "\n",
        "        # Load data from multiple files\n",
        "        for data_dir in data_dirs:\n",
        "            for subdir, dirs, files in os.walk(data_dir):\n",
        "                for file in files:\n",
        "                    file_path = os.path.join(subdir, file)\n",
        "                    data_df = pd.read_csv(file_path)\n",
        "                    dataframes.append(data_df)\n",
        "\n",
        "        merged_data = pd.concat(dataframes, ignore_index=True)\n",
        "\n",
        "        # Apply preprocessing\n",
        "        scaler, fill_values = get_scaler(merged_data)\n",
        "        self.scaler = scaler\n",
        "        self.preprocessed_dfs = [preprocess(scaler, fill_values, df) for df in dataframes]\n",
        "        self.data = [t.tensor(df.values, dtype=t.float32) for df in self.preprocessed_dfs]\n",
        "\n",
        "    def __len__(self):\n",
        "        return sum(len(d) - self.seq_length + 1 for d in self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        data_idx = 0\n",
        "        while index >= len(self.data[data_idx]) - self.seq_length + 1:\n",
        "            index -= len(self.data[data_idx]) - self.seq_length + 1\n",
        "            data_idx += 1\n",
        "\n",
        "        sequence = self.data[data_idx][index:index+self.seq_length]\n",
        "        inputs = sequence[:-1, :]\n",
        "        target = sequence[-1, -1]  # Only predict cbg\n",
        "        return inputs, target\n",
        "\n",
        "def create_dataloader(data_dirs, seq_length, batch_size):\n",
        "    dataset = OhioT1DMDataset(data_dirs, seq_length)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    def unscale(data):\n",
        "        flat = data.reshape(-1, data.shape[-1])\n",
        "        df = pd.DataFrame(flat, columns=dataset.preprocessed_dfs[0].columns)\n",
        "        unscaled = dataset.scaler.inverse_transform(df)\n",
        "        return t.tensor(unscaled).reshape(data.shape)\n",
        "\n",
        "    dataloader.__dict__['unscale'] = unscale\n",
        "    return dataloader\n"
      ],
      "metadata": {
        "id": "MYnXpe2OimFj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTransformerModel(nn.Module):\n",
        "    def __init__(self, input_dim, model_dim, num_heads, num_layers, max_seq_len, output_dim):\n",
        "        super(SimpleTransformerModel, self).__init__()\n",
        "        self.model_dim = model_dim\n",
        "        self.embedding = nn.Linear(input_dim, model_dim)\n",
        "        self.pos_embedding = nn.Parameter(t.randn(1, max_seq_len, model_dim))  # Large enough\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=model_dim,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=4*model_dim,\n",
        "            dropout=0.1,\n",
        "            activation='relu',\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.regressor = nn.Linear(model_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        pos = self.pos_embedding[:, :x.size(1), :]  # Slice to match\n",
        "        x = x + pos\n",
        "        x = self.transformer_encoder(x)\n",
        "        x = x[:, -1, :]\n",
        "        return self.regressor(x)\n"
      ],
      "metadata": {
        "id": "ZWuDL0IRir0o"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloader(data_dirs, seq_length, batch_size, split_val=False, val_split_ratio=0.1):\n",
        "    dataset = OhioT1DMDataset(data_dirs, seq_length)\n",
        "\n",
        "    if split_val:\n",
        "        total_len = len(dataset)\n",
        "        val_len = int(val_split_ratio * total_len)\n",
        "        train_len = total_len - val_len\n",
        "        train_dataset, val_dataset = random_split(dataset, [train_len, val_len])\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "    else:\n",
        "        train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "        val_loader = None\n",
        "\n",
        "    def unscale(data):\n",
        "        flat = data.reshape(-1, 1)\n",
        "        dummy = np.zeros((flat.shape[0], len(dataset.preprocessed_dfs[0].columns)))\n",
        "        dummy[:, 0] = flat.squeeze()\n",
        "        unscaled = dataset.scaler.inverse_transform(dummy)\n",
        "        return t.tensor(unscaled[:, 0]).reshape(data.shape)\n",
        "\n",
        "    train_loader.__dict__['unscale'] = lambda data: unscale(data)\n",
        "    if val_loader:\n",
        "        val_loader.__dict__['unscale'] = lambda data: unscale(data)\n",
        "\n",
        "    return train_loader, val_loader\n"
      ],
      "metadata": {
        "id": "InGwYjoli9F9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_transformer(model, train_loader, val_loader, test_loader, num_epochs=100, lr=0.001):\n",
        "    device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    train_losses, val_losses, test_losses = [], [], []\n",
        "    best_val_loss = float('inf')\n",
        "    best_model = None\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        epoch_train_loss = []\n",
        "\n",
        "        for inputs, targets in train_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs.squeeze(), targets.squeeze())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_train_loss.append(loss.item())\n",
        "\n",
        "        train_losses.append(np.mean(epoch_train_loss))\n",
        "\n",
        "        model.eval()\n",
        "        with t.no_grad():\n",
        "            val_loss = [criterion(model(x.to(device)).squeeze(), y.to(device).squeeze()).item() for x, y in val_loader]\n",
        "            test_loss = [criterion(model(x.to(device)).squeeze(), y.to(device).squeeze()).item() for x, y in test_loader]\n",
        "\n",
        "        val_losses.append(np.mean(val_loss))\n",
        "        test_losses.append(np.mean(test_loss))\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} | Train: {train_losses[-1]:.6f} | Val: {val_losses[-1]:.6f} | Test: {test_losses[-1]:.6f}\")\n",
        "\n",
        "        if val_losses[-1] < best_val_loss:\n",
        "            best_val_loss = val_losses[-1]\n",
        "            best_model = model.state_dict()\n",
        "\n",
        "    model.load_state_dict(best_model)\n",
        "    t.save(model.state_dict(), \"transformer_model.pth\")\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "Cl9tJ1e6jCo_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "path = os.getcwd()\n",
        "\n",
        "train_data_dirs = [\n",
        "    os.path.join(path, \"Ohio Data\", \"Ohio2018_processed\", \"train\"),\n",
        "    os.path.join(path, \"Ohio Data\", \"Ohio2020_processed\", \"train\")\n",
        "]\n",
        "\n",
        "test_data_dirs = [\n",
        "    os.path.join(path, \"Ohio Data\", \"Ohio2018_processed\", \"test\"),\n",
        "    os.path.join(path, \"Ohio Data\", \"Ohio2020_processed\", \"test\")\n",
        "]"
      ],
      "metadata": {
        "id": "ItNrtImojY2N"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adjust input_dim according to your dataset (usually 7)\n",
        "input_dim = 7\n",
        "model_dim = 64\n",
        "num_heads = 4\n",
        "num_layers = 2\n",
        "seq_len = 25\n",
        "output_dim = 1\n",
        "\n",
        "transformer_model = SimpleTransformerModel(input_dim, model_dim, num_heads, num_layers, seq_len, output_dim)\n",
        "\n",
        "# Load data\n",
        "train_loader, val_loader = create_dataloader(train_data_dirs, seq_len, batch_size=500, split_val=True)\n",
        "test_loader, _ = create_dataloader(test_data_dirs, seq_len, batch_size=500, split_val=False)\n",
        "\n",
        "# Train\n",
        "transformer_model = train_transformer(transformer_model, train_loader, val_loader, test_loader, num_epochs=150, lr=0.001)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAtznCbYjKUE",
        "outputId": "3f0adce5-b310-462a-8870-fb9dd58df30a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150 | Train: 0.014086 | Val: 0.000856 | Test: 0.007240\n",
            "Epoch 2/150 | Train: 0.001139 | Val: 0.000184 | Test: 0.000554\n",
            "Epoch 3/150 | Train: 0.000522 | Val: 0.000289 | Test: 0.000397\n",
            "Epoch 4/150 | Train: 0.000378 | Val: 0.000356 | Test: 0.000433\n",
            "Epoch 5/150 | Train: 0.000299 | Val: 0.000084 | Test: 0.000100\n",
            "Epoch 6/150 | Train: 0.000234 | Val: 0.000124 | Test: 0.000170\n",
            "Epoch 7/150 | Train: 0.000192 | Val: 0.000072 | Test: 0.000120\n",
            "Epoch 8/150 | Train: 0.000166 | Val: 0.000043 | Test: 0.000054\n",
            "Epoch 9/150 | Train: 0.000136 | Val: 0.000041 | Test: 0.000045\n",
            "Epoch 10/150 | Train: 0.000114 | Val: 0.000031 | Test: 0.000035\n",
            "Epoch 11/150 | Train: 0.000098 | Val: 0.000032 | Test: 0.000033\n",
            "Epoch 12/150 | Train: 0.000094 | Val: 0.000027 | Test: 0.000034\n",
            "Epoch 13/150 | Train: 0.000079 | Val: 0.000116 | Test: 0.000169\n",
            "Epoch 14/150 | Train: 0.000080 | Val: 0.000108 | Test: 0.000096\n",
            "Epoch 15/150 | Train: 0.000078 | Val: 0.000018 | Test: 0.000019\n",
            "Epoch 16/150 | Train: 0.000071 | Val: 0.000016 | Test: 0.000016\n",
            "Epoch 17/150 | Train: 0.000063 | Val: 0.000039 | Test: 0.000066\n",
            "Epoch 18/150 | Train: 0.000057 | Val: 0.000059 | Test: 0.000109\n",
            "Epoch 19/150 | Train: 0.000061 | Val: 0.000016 | Test: 0.000020\n",
            "Epoch 20/150 | Train: 0.000053 | Val: 0.000013 | Test: 0.000023\n",
            "Epoch 21/150 | Train: 0.000051 | Val: 0.000015 | Test: 0.000030\n",
            "Epoch 22/150 | Train: 0.000052 | Val: 0.000061 | Test: 0.000110\n",
            "Epoch 23/150 | Train: 0.000050 | Val: 0.000017 | Test: 0.000016\n",
            "Epoch 24/150 | Train: 0.000058 | Val: 0.000018 | Test: 0.000012\n",
            "Epoch 25/150 | Train: 0.000044 | Val: 0.000017 | Test: 0.000043\n",
            "Epoch 26/150 | Train: 0.000038 | Val: 0.000054 | Test: 0.000071\n",
            "Epoch 27/150 | Train: 0.000041 | Val: 0.000010 | Test: 0.000010\n",
            "Epoch 28/150 | Train: 0.000048 | Val: 0.000025 | Test: 0.000061\n",
            "Epoch 29/150 | Train: 0.000038 | Val: 0.000018 | Test: 0.000057\n",
            "Epoch 30/150 | Train: 0.000039 | Val: 0.000018 | Test: 0.000023\n",
            "Epoch 31/150 | Train: 0.000044 | Val: 0.000022 | Test: 0.000065\n",
            "Epoch 32/150 | Train: 0.000037 | Val: 0.000012 | Test: 0.000014\n",
            "Epoch 33/150 | Train: 0.000034 | Val: 0.000018 | Test: 0.000037\n",
            "Epoch 34/150 | Train: 0.000033 | Val: 0.000068 | Test: 0.000047\n",
            "Epoch 35/150 | Train: 0.000044 | Val: 0.000011 | Test: 0.000010\n",
            "Epoch 36/150 | Train: 0.000031 | Val: 0.000009 | Test: 0.000014\n",
            "Epoch 37/150 | Train: 0.000038 | Val: 0.000018 | Test: 0.000017\n",
            "Epoch 38/150 | Train: 0.000032 | Val: 0.000009 | Test: 0.000011\n",
            "Epoch 39/150 | Train: 0.000028 | Val: 0.000020 | Test: 0.000015\n",
            "Epoch 40/150 | Train: 0.000032 | Val: 0.000012 | Test: 0.000013\n",
            "Epoch 41/150 | Train: 0.000026 | Val: 0.000043 | Test: 0.000034\n",
            "Epoch 42/150 | Train: 0.000036 | Val: 0.000009 | Test: 0.000009\n",
            "Epoch 43/150 | Train: 0.000030 | Val: 0.000013 | Test: 0.000012\n",
            "Epoch 44/150 | Train: 0.000078 | Val: 0.000009 | Test: 0.000011\n",
            "Epoch 45/150 | Train: 0.000017 | Val: 0.000028 | Test: 0.000052\n",
            "Epoch 46/150 | Train: 0.000017 | Val: 0.000013 | Test: 0.000031\n",
            "Epoch 47/150 | Train: 0.000020 | Val: 0.000012 | Test: 0.000009\n",
            "Epoch 48/150 | Train: 0.000020 | Val: 0.000071 | Test: 0.000120\n",
            "Epoch 49/150 | Train: 0.000026 | Val: 0.000047 | Test: 0.000042\n",
            "Epoch 50/150 | Train: 0.000025 | Val: 0.000026 | Test: 0.000026\n",
            "Epoch 51/150 | Train: 0.000493 | Val: 0.000076 | Test: 0.000373\n",
            "Epoch 52/150 | Train: 0.000046 | Val: 0.000011 | Test: 0.000012\n",
            "Epoch 53/150 | Train: 0.000019 | Val: 0.000012 | Test: 0.000017\n",
            "Epoch 54/150 | Train: 0.000017 | Val: 0.000011 | Test: 0.000014\n",
            "Epoch 55/150 | Train: 0.000015 | Val: 0.000008 | Test: 0.000008\n",
            "Epoch 56/150 | Train: 0.000014 | Val: 0.000014 | Test: 0.000022\n",
            "Epoch 57/150 | Train: 0.000015 | Val: 0.000008 | Test: 0.000008\n",
            "Epoch 58/150 | Train: 0.000013 | Val: 0.000008 | Test: 0.000008\n",
            "Epoch 59/150 | Train: 0.000013 | Val: 0.000007 | Test: 0.000007\n",
            "Epoch 60/150 | Train: 0.000015 | Val: 0.000011 | Test: 0.000016\n",
            "Epoch 61/150 | Train: 0.000018 | Val: 0.000008 | Test: 0.000009\n",
            "Epoch 62/150 | Train: 0.000019 | Val: 0.000009 | Test: 0.000009\n",
            "Epoch 63/150 | Train: 0.000017 | Val: 0.000022 | Test: 0.000020\n",
            "Epoch 64/150 | Train: 0.000017 | Val: 0.000009 | Test: 0.000007\n",
            "Epoch 65/150 | Train: 0.000034 | Val: 0.000008 | Test: 0.000008\n",
            "Epoch 66/150 | Train: 0.000013 | Val: 0.000007 | Test: 0.000009\n",
            "Epoch 67/150 | Train: 0.000015 | Val: 0.000007 | Test: 0.000008\n",
            "Epoch 68/150 | Train: 0.000018 | Val: 0.000007 | Test: 0.000010\n",
            "Epoch 69/150 | Train: 0.000016 | Val: 0.000011 | Test: 0.000012\n",
            "Epoch 70/150 | Train: 0.000017 | Val: 0.000016 | Test: 0.000022\n",
            "Epoch 71/150 | Train: 0.000018 | Val: 0.000007 | Test: 0.000007\n",
            "Epoch 72/150 | Train: 0.000017 | Val: 0.000007 | Test: 0.000007\n",
            "Epoch 73/150 | Train: 0.000021 | Val: 0.000014 | Test: 0.000048\n",
            "Epoch 74/150 | Train: 0.000030 | Val: 0.000006 | Test: 0.000008\n",
            "Epoch 75/150 | Train: 0.000010 | Val: 0.000008 | Test: 0.000008\n",
            "Epoch 76/150 | Train: 0.000013 | Val: 0.000007 | Test: 0.000006\n",
            "Epoch 77/150 | Train: 0.000176 | Val: 0.000008 | Test: 0.000010\n",
            "Epoch 78/150 | Train: 0.000013 | Val: 0.000007 | Test: 0.000007\n",
            "Epoch 79/150 | Train: 0.000011 | Val: 0.000007 | Test: 0.000007\n",
            "Epoch 80/150 | Train: 0.000010 | Val: 0.000006 | Test: 0.000007\n",
            "Epoch 81/150 | Train: 0.000009 | Val: 0.000007 | Test: 0.000010\n",
            "Epoch 82/150 | Train: 0.000009 | Val: 0.000012 | Test: 0.000010\n",
            "Epoch 83/150 | Train: 0.000009 | Val: 0.000006 | Test: 0.000011\n",
            "Epoch 84/150 | Train: 0.000011 | Val: 0.000006 | Test: 0.000006\n",
            "Epoch 85/150 | Train: 0.000014 | Val: 0.000006 | Test: 0.000008\n",
            "Epoch 86/150 | Train: 0.000014 | Val: 0.000007 | Test: 0.000006\n",
            "Epoch 87/150 | Train: 0.000014 | Val: 0.000024 | Test: 0.000034\n",
            "Epoch 88/150 | Train: 0.000016 | Val: 0.000011 | Test: 0.000010\n",
            "Epoch 89/150 | Train: 0.000014 | Val: 0.000006 | Test: 0.000006\n",
            "Epoch 90/150 | Train: 0.000014 | Val: 0.000006 | Test: 0.000009\n",
            "Epoch 91/150 | Train: 0.000016 | Val: 0.000007 | Test: 0.000007\n",
            "Epoch 92/150 | Train: 0.000014 | Val: 0.000010 | Test: 0.000006\n",
            "Epoch 93/150 | Train: 0.000013 | Val: 0.000012 | Test: 0.000008\n",
            "Epoch 94/150 | Train: 0.000014 | Val: 0.000008 | Test: 0.000010\n",
            "Epoch 95/150 | Train: 0.000014 | Val: 0.000014 | Test: 0.000017\n",
            "Epoch 96/150 | Train: 0.000014 | Val: 0.000009 | Test: 0.000007\n",
            "Epoch 97/150 | Train: 0.000014 | Val: 0.000006 | Test: 0.000010\n",
            "Epoch 98/150 | Train: 0.000014 | Val: 0.000006 | Test: 0.000007\n",
            "Epoch 99/150 | Train: 0.000014 | Val: 0.000015 | Test: 0.000031\n",
            "Epoch 100/150 | Train: 0.000014 | Val: 0.000006 | Test: 0.000007\n",
            "Epoch 101/150 | Train: 0.000054 | Val: 0.000008 | Test: 0.000007\n",
            "Epoch 102/150 | Train: 0.000007 | Val: 0.000006 | Test: 0.000009\n",
            "Epoch 103/150 | Train: 0.000007 | Val: 0.000006 | Test: 0.000005\n",
            "Epoch 104/150 | Train: 0.000007 | Val: 0.000005 | Test: 0.000006\n",
            "Epoch 105/150 | Train: 0.000011 | Val: 0.000014 | Test: 0.000012\n",
            "Epoch 106/150 | Train: 0.000012 | Val: 0.000007 | Test: 0.000006\n",
            "Epoch 107/150 | Train: 0.000012 | Val: 0.000009 | Test: 0.000015\n",
            "Epoch 108/150 | Train: 0.000015 | Val: 0.000008 | Test: 0.000010\n",
            "Epoch 109/150 | Train: 0.000013 | Val: 0.000006 | Test: 0.000006\n",
            "Epoch 110/150 | Train: 0.000021 | Val: 0.000006 | Test: 0.000009\n",
            "Epoch 111/150 | Train: 0.000008 | Val: 0.000005 | Test: 0.000006\n",
            "Epoch 112/150 | Train: 0.000014 | Val: 0.000014 | Test: 0.000020\n",
            "Epoch 113/150 | Train: 0.000011 | Val: 0.000008 | Test: 0.000006\n",
            "Epoch 114/150 | Train: 0.000012 | Val: 0.000006 | Test: 0.000005\n",
            "Epoch 115/150 | Train: 0.000032 | Val: 0.000005 | Test: 0.000007\n",
            "Epoch 116/150 | Train: 0.000006 | Val: 0.000006 | Test: 0.000005\n",
            "Epoch 117/150 | Train: 0.000007 | Val: 0.000005 | Test: 0.000006\n",
            "Epoch 118/150 | Train: 0.000012 | Val: 0.000020 | Test: 0.000014\n",
            "Epoch 119/150 | Train: 0.000011 | Val: 0.000006 | Test: 0.000006\n",
            "Epoch 120/150 | Train: 0.000013 | Val: 0.000005 | Test: 0.000006\n",
            "Epoch 121/150 | Train: 0.000012 | Val: 0.000023 | Test: 0.000019\n",
            "Epoch 122/150 | Train: 0.000012 | Val: 0.000006 | Test: 0.000007\n",
            "Epoch 123/150 | Train: 0.000013 | Val: 0.000006 | Test: 0.000011\n",
            "Epoch 124/150 | Train: 0.000012 | Val: 0.000007 | Test: 0.000012\n",
            "Epoch 125/150 | Train: 0.000014 | Val: 0.000006 | Test: 0.000008\n",
            "Epoch 126/150 | Train: 0.000011 | Val: 0.000006 | Test: 0.000005\n",
            "Epoch 127/150 | Train: 0.000011 | Val: 0.000014 | Test: 0.000021\n",
            "Epoch 128/150 | Train: 0.000078 | Val: 0.000006 | Test: 0.000006\n",
            "Epoch 129/150 | Train: 0.000006 | Val: 0.000005 | Test: 0.000006\n",
            "Epoch 130/150 | Train: 0.000006 | Val: 0.000008 | Test: 0.000013\n",
            "Epoch 131/150 | Train: 0.000006 | Val: 0.000005 | Test: 0.000007\n",
            "Epoch 132/150 | Train: 0.000007 | Val: 0.000005 | Test: 0.000009\n",
            "Epoch 133/150 | Train: 0.000007 | Val: 0.000013 | Test: 0.000008\n",
            "Epoch 134/150 | Train: 0.000012 | Val: 0.000005 | Test: 0.000006\n",
            "Epoch 135/150 | Train: 0.000012 | Val: 0.000009 | Test: 0.000006\n",
            "Epoch 136/150 | Train: 0.000011 | Val: 0.000005 | Test: 0.000006\n",
            "Epoch 137/150 | Train: 0.000012 | Val: 0.000024 | Test: 0.000015\n",
            "Epoch 138/150 | Train: 0.000011 | Val: 0.000006 | Test: 0.000006\n",
            "Epoch 139/150 | Train: 0.000011 | Val: 0.000005 | Test: 0.000007\n",
            "Epoch 140/150 | Train: 0.000012 | Val: 0.000005 | Test: 0.000006\n",
            "Epoch 141/150 | Train: 0.000012 | Val: 0.000010 | Test: 0.000016\n",
            "Epoch 142/150 | Train: 0.000010 | Val: 0.000006 | Test: 0.000010\n",
            "Epoch 143/150 | Train: 0.000013 | Val: 0.000007 | Test: 0.000007\n",
            "Epoch 144/150 | Train: 0.000011 | Val: 0.000010 | Test: 0.000008\n",
            "Epoch 145/150 | Train: 0.000011 | Val: 0.000005 | Test: 0.000007\n",
            "Epoch 146/150 | Train: 0.000011 | Val: 0.000007 | Test: 0.000007\n",
            "Epoch 147/150 | Train: 0.000010 | Val: 0.000005 | Test: 0.000007\n",
            "Epoch 148/150 | Train: 0.000013 | Val: 0.000007 | Test: 0.000006\n",
            "Epoch 149/150 | Train: 0.000011 | Val: 0.000005 | Test: 0.000006\n",
            "Epoch 150/150 | Train: 0.000060 | Val: 0.000007 | Test: 0.000007\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch as t\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "def evaluate_model(model, dataloader):\n",
        "    device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    y_true, y_pred = [], []\n",
        "\n",
        "    with t.no_grad():\n",
        "        for inputs, targets in dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            y_true.extend(targets.squeeze().cpu().numpy())\n",
        "            y_pred.extend(outputs.squeeze().cpu().numpy())\n",
        "\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "    return mse, rmse, mae, r2\n"
      ],
      "metadata": {
        "id": "0wSEUvgowkQu"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mse, rmse, mae, r2 = evaluate_model(transformer_model, val_loader)\n",
        "print(f\"MSE: {mse:.4f}\\nRMSE: {rmse:.4f}\\nMAE: {mae:.4f}\\nR2 Score: {r2:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPthTEvAw7nk",
        "outputId": "639b89e5-ecb8-44b2-a251-667177cd28d2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 0.0000\n",
            "RMSE: 0.0027\n",
            "MAE: 0.0018\n",
            "R2 Score: 0.9958\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model state_dict\n",
        "t.save(transformer_model.state_dict(), 'transformer_model.pth')\n"
      ],
      "metadata": {
        "id": "2JFXbMJN9ii0"
      },
      "execution_count": 18,
      "outputs": []
    }
  ]
}